{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This notebook__ showcases how to infer a trained WaveStudent obtained using distill_waveglow.py. It requires the same environment as was used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=7\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import Audio, display, clear_output\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "\n",
    "import librosa # was: 0.7.2\n",
    "from pathlib import Path\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from mel2samp import Mel2Samp, load_wav_to_torch\n",
    "from models import defaults\n",
    "from denoiser import StudentDenoiser, TeacherDenoiser\n",
    "\n",
    "np.random.seed(1337)\n",
    "torch.manual_seed(1337)\n",
    "torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_wav(audio, path):\n",
    "    wavfile.write(\n",
    "        \"tmp.wav\", defaults.SAMPLING_RATE, \n",
    "        (np.clip(audio, -1, 1) * 32767).astype(np.int16)\n",
    "    )\n",
    "    !sox \"tmp.wav\" {path} norm -1\n",
    "    !rm \"tmp.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_kwargs = dict(\n",
    "    split='test',\n",
    "    segment_length=None,\n",
    "    filter_length=defaults.STFT_FILTER_LENGTH,\n",
    "    hop_length=defaults.STFT_HOP_LENGTH,\n",
    "    win_length=defaults.STFT_WIN_LENGTH,\n",
    "    sampling_rate=defaults.SAMPLING_RATE,\n",
    "    mel_fmin=defaults.MEL_FMIN,\n",
    "    mel_fmax=defaults.MEL_FMAX\n",
    ")\n",
    "test_dataset = Mel2Samp(\"../data/wavs\", **dataset_kwargs)\n",
    "\n",
    "test_loader_kwargs = dict(\n",
    "    num_workers=1, shuffle=False,\n",
    "    batch_size=1, pin_memory=False\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, **test_loader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../pretrained_models'\n",
    "sample_path = '../samples'\n",
    "                            \n",
    "sigma = 1.0\n",
    "device = 'cuda'\n",
    "dtype = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cast WaveGlow to fp16\n"
     ]
    }
   ],
   "source": [
    "from models.waveglow_teacher import WaveGlowTeacher\n",
    "teacher_path = os.path.join(model_path, 'wg_teacher_ch256_wn12.pth')\n",
    "teacher = WaveGlowTeacher.load(teacher_path, fp16=(dtype==torch.float16)).train(False).to(device)\n",
    "teacher_denoiser = TeacherDenoiser(teacher, mode='zeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create student models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from denoiser import StudentDenoiser\n",
    "from models.students import WaveNetStudent, WideFlowStudent, FlowStudent, AffineStudent\n",
    "\n",
    "\n",
    "def create_student(model_path, ch=96, wn=4, student_arch='wg'):\n",
    "    if student_arch == 'wg':\n",
    "        student = WaveNetStudent(in_channels=8, mel_channels=640, hid_channels=ch, n_wavenets=wn,\n",
    "                          wavenet_layers=8, kernel_size=3).to(device).train(True)\n",
    "    elif student_arch == 'flow':\n",
    "        student = FlowStudent(in_channels=8, mel_channels=640, hid_channels=ch, n_wavenets=wn,\n",
    "                              wavenet_layers=8, kernel_size=3).to(device).train(True)\n",
    "    elif student_arch == 'wide_flow':\n",
    "        student = WideFlowStudent(in_channels=8, mel_channels=640, hid_channels=ch, n_wavenets=wn,\n",
    "                              wavenet_layers=8, kernel_size=3).to(device).train(True)\n",
    "    elif student_arch == 'affine':\n",
    "        student = AffineStudent(in_channels=8, mel_channels=640, hid_channels=ch, n_wavenets=wn,\n",
    "                                wavenet_layers=8, kernel_size=3).to(device).train(True)\n",
    "    \n",
    "    ckpt_path = os.path.join(model_path, f'{student_arch}_student_ch{ch}_wn{wn}.pth')\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "    student.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    if student_arch in ['flow', 'affine']:\n",
    "        # FIXME: do not add dummy params \n",
    "        student.in_proj = nn.Linear(1, 1).to(device).to(dtype)\n",
    "    \n",
    "    def remove(conv_list):\n",
    "        new_conv_list = torch.nn.ModuleList()\n",
    "        for old_conv in conv_list:\n",
    "            old_conv = torch.nn.utils.remove_weight_norm(old_conv)\n",
    "            new_conv_list.append(old_conv)\n",
    "        return new_conv_list\n",
    "\n",
    "    wavenets = student.WN if hasattr(student, \"WN\") else student.wavenets\n",
    "    for wn in wavenets:\n",
    "        wn.start = torch.nn.utils.remove_weight_norm(wn.start)\n",
    "        wn.in_layers = remove(wn.in_layers)\n",
    "        wn.cond_layers = torch.nn.utils.remove_weight_norm(wn.cond_layer)\n",
    "        wn.res_skip_layers = remove(wn.res_skip_layers)\n",
    "    return student.train(False).to(dtype).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = {}\n",
    "students['wg_ch96_wn2'] = create_student(model_path, ch=96, wn=2)\n",
    "students['wg_ch96_wn4'] = create_student(model_path, ch=96, wn=4)\n",
    "students['wg_ch128_wn4'] = create_student(model_path, ch=128, wn=4)\n",
    "students['flow_ch96_wn4'] = create_student(model_path, ch=96, wn=4, student_arch='flow')\n",
    "students['wide_flow_sh96_wn4'] = create_student(model_path, ch=96, wn=4, student_arch='wide_flow')\n",
    "students['affine_sh96_wn4'] = create_student(model_path, ch=96, wn=4, student_arch='affine')\n",
    "\n",
    "student_denoisers = {}\n",
    "for key, student in students.items():\n",
    "    student_denoisers[key] = StudentDenoiser(student, teacher, mode='zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../mel2samp.py:74: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sampling_rate, data = read(full_path)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e1dade755e4a5dbaa5fafcaf7c30e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../mel2samp.py:74: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sampling_rate, data = read(full_path)\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "result_path = os.path.join(sample_path, 'release_sigma_1')\n",
    "!rm -rf {result_path}\n",
    "!mkdir -p {result_path}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "for sample_i, (mel, wav) in enumerate(tqdm(test_loader)):\n",
    "    sample_path = test_dataset.audio_files[sample_i]\n",
    "    sample_id = str(sample_path).split('/')[-1].split('.')[0]\n",
    "    mel = mel.to(dtype).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ### Teacher ###\n",
    "        inputs = teacher.sample_inputs_for(mel, sigma=sigma)\n",
    "        teacher_prediction = teacher(*inputs)\n",
    "        while (teacher_prediction < -1).any() or (teacher_prediction > 1).any():\n",
    "            inputs = teacher.sample_inputs_for(mel, sigma=sigma)\n",
    "            teacher_prediction = teacher(*inputs)\n",
    "\n",
    "        teacher_prediction = teacher_denoiser(teacher_prediction, strength=0.004)\n",
    "        teacher_audio = teacher_prediction.reshape(-1).clamp(-1, 1).data.cpu().float().numpy()\n",
    "\n",
    "        ### Students ###\n",
    "        student_mel = inputs[0]\n",
    "        student_audios = {}\n",
    "        for key in students.keys():\n",
    "            student_input = torch.cat(inputs[1:], dim=1)\n",
    "            student_prediction = students[key](student_input, student_mel).permute(0, 2, 1).flatten(1)\n",
    "            student_prediction = student_denoisers[key](student_prediction, strength=0.004) \n",
    "            student_audios[key] = student_prediction.reshape(-1).clamp(-1, 1).data.cpu().float().numpy()\n",
    "\n",
    "    path = os.path.join(result_path, 'teacher')\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    save_wav(teacher_audio, os.path.join(path, f\"{sample_id}.wav\"))\n",
    "\n",
    "    for i, (key, student_audio) in enumerate(student_audios.items()):        \n",
    "        path = os.path.join(result_path, f'{key}')\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        save_wav(student_audio, os.path.join(path, f\"{sample_id}.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate inference speed in MHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_teacher_inference_speed(model, denoiser, repetitions=50):\n",
    "    model.eval()\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    timings=np.zeros((repetitions,1))\n",
    "    \n",
    "    mel = test_dataset[0][0]\n",
    "    inputs = model.sample_inputs_for(mel.unsqueeze(0).half().cuda())\n",
    "    inputs = [input.repeat(32, 1, 2) for input in inputs]\n",
    "    \n",
    "    #GPU-WARM-UP\n",
    "    for _ in range(10):\n",
    "        output = denoiser(model(*inputs))\n",
    "    num_samples = np.prod(output.shape)\n",
    "\n",
    "    # MEASURE PERFORMANCE\n",
    "    for rep in tqdm(range(repetitions)):\n",
    "        starter.record()\n",
    "        denoiser(model(*inputs))\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "    mean_syn = np.sum(timings) / repetitions\n",
    "    std_syn = np.std(timings)\n",
    "    mhz = num_samples / mean_syn / 1000\n",
    "    return mhz\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_student_inference_speed(model, denoiser, repetitions=50):\n",
    "    model.eval()\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    timings=np.zeros((repetitions,1))\n",
    "    \n",
    "    mel = test_dataset[0][0]\n",
    "    inputs = teacher.sample_inputs_for(mel.unsqueeze(0).half().cuda())\n",
    "    inputs = [input.repeat(32, 1, 2) for input in inputs]\n",
    "    student_mel = inputs[0]\n",
    "    student_input = torch.cat(inputs[1:], dim=1)\n",
    "    \n",
    "    #GPU-WARM-UP\n",
    "    for _ in range(10):\n",
    "        output = denoiser(model(student_input, student_mel).permute(0, 2, 1).flatten(1))\n",
    "    num_samples = np.prod(output.shape)\n",
    "    \n",
    "    # MEASURE PERFORMANCE\n",
    "    for rep in tqdm(range(repetitions)):\n",
    "        starter.record()\n",
    "        denoiser(model(student_input, student_mel).permute(0, 2, 1).flatten(1))\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "    mean_syn = np.sum(timings) / repetitions\n",
    "    std_syn = np.std(timings)\n",
    "    mhz = num_samples / mean_syn / 1000\n",
    "    return mhz\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_melgan_inference_speed(model, repetitions=50):\n",
    "    model.mel2wav.eval()\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    timings=np.zeros((repetitions,1))\n",
    "    \n",
    "    wav = test_dataset[0][1]\n",
    "    melgan_mel = model(wav.unsqueeze(0)).repeat(32, 1, 2).half()\n",
    "    \n",
    "    #GPU-WARM-UP\n",
    "    for _ in range(10):\n",
    "        output = melgan.inverse(melgan_mel)\n",
    "    num_samples = np.prod(output.shape)\n",
    "\n",
    "    # MEASURE PERFORMANCE\n",
    "    for rep in tqdm(range(repetitions)):\n",
    "        starter.record()\n",
    "        out = melgan.inverse(melgan_mel)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "    mean_syn = np.sum(timings) / repetitions\n",
    "    std_syn = np.std(timings)\n",
    "    mhz = num_samples / mean_syn / 1000\n",
    "    return mhz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhz = get_teacher_inference_speed(teacher, teacher_denoiser)\n",
    "print(f\"Teacher MHz: {mhz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, student in students.items():\n",
    "    mhz = get_student_inference_speed(student, student_denoisers[key])\n",
    "    print(f\"{key} MHz: {mhz}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
